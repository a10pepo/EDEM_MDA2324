{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3WKxUMvQ9JW"
      },
      "source": [
        "# DataFrames Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBsRl4yARA3x"
      },
      "source": [
        "## Prerrequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KID1ObYRDA4"
      },
      "source": [
        "Install Spark and Java in VM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-9EjQzqhRJSZ"
      },
      "outputs": [],
      "source": [
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark 3.5.0\n",
        "!wget -q https://apache.osuosl.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnaoMgaPuUuD",
        "outputId": "0bf6c6ad-3a54-48f0-fb37-3fb81e999631"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 391016\n",
            "drwxr-xr-x 1 root root      4096 Jan 10 14:23 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r-- 1 root root 400395283 Sep  9 02:10 spark-3.5.0-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4s6wcUsSPHQ",
        "outputId": "7b5bcae6-0f99-4ac3-c7a2-0f4c849d9914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 391016\n",
            "drwxr-xr-x 1 root root      4096 Jan 10 14:23 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r-- 1 root root 400395283 Sep  9 02:10 spark-3.5.0-bin-hadoop3.tgz\n"
          ]
        }
      ],
      "source": [
        "ls -l # check the .tgz is there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "hSA_T2q7SEt_"
      },
      "outputs": [],
      "source": [
        "# unzip it\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HpKEfJTeii2Y"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwH-zC17SGnP",
        "outputId": "63fc725d-8050-4f8a-dc75-a7a98b390aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from folium) (0.7.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from folium) (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from folium) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from folium) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9->folium) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2023.11.17)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install py4j\n",
        "\n",
        "# For maps\n",
        "!pip install folium\n",
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1tk452JRjuY"
      },
      "source": [
        "Define the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vrMxCiuZRl7h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_3rL02Q9JZ"
      },
      "source": [
        "Start Spark Session\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-HtQz6mfQ9JZ",
        "outputId": "64904c28-7903-4767-ea2c-daf31fddaecc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.5.0-bin-hadoop3\")# SPARK_HOME\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create the session\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"DataFrames Basics\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "DrOjmfL9Q9Ja",
        "outputId": "315ffbe4-9c30-4aee-ef24-7ca0e4ef4398"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e4dc2380400>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://324d18605b77:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DataFrames Basics</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IBZ8ufPAQ9Jb"
      },
      "outputs": [],
      "source": [
        "# For Pandas conversion optimization\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nJ-cdrAlQ9Jb"
      },
      "outputs": [],
      "source": [
        "# Import sql functions\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIP7U3YYTDbw"
      },
      "source": [
        "Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "d63cazZLTKTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304dd5e4-6bc4-4795-b252-e4340ad5f4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bank.csv  cars.json  movies.json  vehicles.csv\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p dataset\n",
        "!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/cars.json -P /dataset\n",
        "!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/movies.json -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/bank.csv -P /dataset\n",
        "!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/vehicles.csv -P /dataset\n",
        "!ls /dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml1O2OiQU6ah",
        "outputId": "f77afa31-39e5-40c0-d191-22c6e664a8e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1784\n",
            "-rw-r--r-- 1 root root  461474 Jan 12 18:01 bank.csv\n",
            "-rw-r--r-- 1 root root   74910 Jan 12 18:01 cars.json\n",
            "-rw-r--r-- 1 root root 1274347 Jan 12 18:01 movies.json\n",
            "-rw-r--r-- 1 root root    4370 Jan 12 18:01 vehicles.csv\n"
          ]
        }
      ],
      "source": [
        "ls -l /dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94aRTBiPQ9Jc"
      },
      "source": [
        "## Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "XxXiDT4XUgjl"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p69TjjJBTrDF",
        "outputId": "f07c2753-3c90-452b-9e35-cacf71dc9c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-------+---------+-------+\n",
            "|age|       job|marital|education|balance|\n",
            "+---+----------+-------+---------+-------+\n",
            "| 30|unemployed|married|  primary|   1787|\n",
            "| 33|  services|married|secondary|   4789|\n",
            "| 35|management| single| tertiary|   1350|\n",
            "+---+----------+-------+---------+-------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bankText = spark.sparkContext.textFile(\"/dataset/bank.csv\")\n",
        "\n",
        "#we have to: remove firt row (headers), map the rest, and create DF\n",
        "bank = bankText.map(lambda lineaCsv: lineaCsv.split(\";\"))\\\n",
        ".filter(lambda s: s[0] != \"\\\"age\\\"\") \\\n",
        ".map(lambda row: Row(int(row[0]), row[1].replace(\"\\\"\", \"\"), row[2].replace(\"\\\"\", \"\"), row[3].replace(\"\\\"\", \"\"), row[5].replace(\"\\\"\", \"\"))) \\\n",
        ".toDF([\"age\", \"job\", \"marital\", \"education\", \"balance\"]) \\\n",
        ".withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
        "\n",
        "bank.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnCG9XHsTUfe"
      },
      "source": [
        "Read directly from JSON file to a DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "iBWBcsLDTYuF"
      },
      "outputs": [],
      "source": [
        "carsDF = spark.read.option(\"inferSchema\", True).json(\"/dataset/cars.json\") # inferSchema requires one extra pass over the data\n",
        "\n",
        "# if None is set, it uses de default value (default = False) you can also pass the schema manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beN35oUrUo4c"
      },
      "source": [
        "Read directly from csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaxDO13vUlSs",
        "outputId": "ef8aa2ac-b383-4652-f321-3b131646322b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "|age|       job|marital|education|default|balance|housing|loan| contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "| 30|unemployed|married|  primary|     no|   1787|     no|  no|cellular| 19|  oct|      79|       1|   -1|       0| unknown| no|\n",
            "| 33|  services|married|secondary|     no|   4789|    yes| yes|cellular| 11|  may|     220|       1|  339|       4| failure| no|\n",
            "| 35|management| single| tertiary|     no|   1350|    yes|  no|cellular| 16|  apr|     185|       1|  330|       1| failure| no|\n",
            "+---+----------+-------+---------+-------+-------+-------+----+--------+---+-----+--------+--------+-----+--------+--------+---+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bankDF = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(\"/dataset/bank.csv\")\n",
        "bankDF.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOt52pbaQ9Jc"
      },
      "source": [
        "Showing a DF and print schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmmsrRxaQ9Jd",
        "outputId": "285096d9-fd89-444c-896f-e531129d0dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
            "|Acceleration|Cylinders|Displacement|Horsepower|Miles_per_Gallon|                Name|Origin|Weight_in_lbs|      Year|\n",
            "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
            "|        12.0|        8|       307.0|       130|            18.0|chevrolet chevell...|   USA|         3504|1970-01-01|\n",
            "|        11.5|        8|       350.0|       165|            15.0|   buick skylark 320|   USA|         3693|1970-01-01|\n",
            "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
            "only showing top 2 rows\n",
            "\n",
            "root\n",
            " |-- Acceleration: double (nullable = true)\n",
            " |-- Cylinders: long (nullable = true)\n",
            " |-- Displacement: double (nullable = true)\n",
            " |-- Horsepower: long (nullable = true)\n",
            " |-- Miles_per_Gallon: double (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            " |-- Weight_in_lbs: long (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "carsDF.show(2)\n",
        "carsDF.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij4TDMZ0lkBZ"
      },
      "source": [
        "Get Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB06W84wlnlY",
        "outputId": "0c5ffd8e-0dd1-48a3-de71-0167a14412f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Acceleration=12.0, Cylinders=8, Displacement=307.0, Horsepower=130, Miles_per_Gallon=18.0, Name='chevrolet chevelle malibu', Origin='USA', Weight_in_lbs=3504, Year='1970-01-01'),\n",
              " Row(Acceleration=11.5, Cylinders=8, Displacement=350.0, Horsepower=165, Miles_per_Gallon=15.0, Name='buick skylark 320', Origin='USA', Weight_in_lbs=3693, Year='1970-01-01')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "carsDF.take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z89eHF1aQ9Jd"
      },
      "source": [
        "Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtFilGlZWGyo",
        "outputId": "f2a6cef8-dd64-4d15-acf8-3cb447987e60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "406"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "carsDF.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akmA70EPQ9Jd"
      },
      "source": [
        "Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp6hKanzQ9Je",
        "outputId": "cd24a12e-4eb3-4421-8d56-a5e681817a6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.types.StructType'>\n",
            "StructType([StructField('Acceleration', DoubleType(), True), StructField('Cylinders', LongType(), True), StructField('Displacement', DoubleType(), True), StructField('Horsepower', LongType(), True), StructField('Miles_per_Gallon', DoubleType(), True), StructField('Name', StringType(), True), StructField('Origin', StringType(), True), StructField('Weight_in_lbs', LongType(), True), StructField('Year', StringType(), True)])\n"
          ]
        }
      ],
      "source": [
        "# obtain a schema\n",
        "carsSchema = carsDF.schema\n",
        "print(type(carsSchema))\n",
        "print(carsSchema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_XS0vB0Q9Je"
      },
      "source": [
        "Custom Schemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ekQkJLT8Q9Je"
      },
      "outputs": [],
      "source": [
        "example = spark.sparkContext.parallelize([(\"chevrolet chevelle malibu\",18,\"1970-01-01\",\"USA\"),\n",
        "    (\"buick skylark 320\",15,\"1970-01-01\",\"USA\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUzjX4kTQ9Je",
        "outputId": "415d322d-f242-4d10-fc3d-7be364738f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: string (nullable = true)\n",
            " |-- _2: long (nullable = true)\n",
            " |-- _3: string (nullable = true)\n",
            " |-- _4: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "exampleDF = spark.createDataFrame(example)\n",
        "exampleDF.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeTtjRirQ9Je"
      },
      "source": [
        "With columns names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "uhY-f3j1Q9Jf"
      },
      "outputs": [],
      "source": [
        "names = list([\"name\", \"weight\", \"date\", \"country\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqxqPyr1Q9Jf",
        "outputId": "11e8cee0-b607-4656-90c5-8c08b1aa87cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- weight: long (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example2DF = example.toDF(names)\n",
        "example2DF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "toUIbosSQ9Jf"
      },
      "outputs": [],
      "source": [
        "# importing sql types\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KZNfLhFWQ9Jf"
      },
      "outputs": [],
      "source": [
        "# custom schema\n",
        "customSchema = StructType([ \\\n",
        "    StructField('name', StringType(), True), \\\n",
        "    StructField('weight', StringType(), True), \\\n",
        "    StructField('date', StringType(), True), \\\n",
        "    StructField('country', StringType(), True)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x63bbTDYQ9Jf",
        "outputId": "a842ff0f-8c6f-4fbd-d348-766ad1266505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- weight: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example3DF = spark.createDataFrame(example, customSchema)\n",
        "example3DF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRiqa-G5Q9Jf",
        "outputId": "5d899edc-e6bb-494d-fe54-c713cf08411d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------+------+----------+-------+\n",
            "|name                     |weight|date      |country|\n",
            "+-------------------------+------+----------+-------+\n",
            "|chevrolet chevelle malibu|18    |1970-01-01|USA    |\n",
            "|buick skylark 320        |15    |1970-01-01|USA    |\n",
            "+-------------------------+------+----------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example3DF.show(2, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "M3qToymLQ9Jg"
      },
      "outputs": [],
      "source": [
        "# we can also specify schema with DDL (Data Definition Language)\n",
        "customSchema2 = \"`name` STRING NOT NULL, `weight` INT, `date` STRING, `country` STRING\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k9X1gSSQ9Jg",
        "outputId": "8babc57a-069d-4e90-b25a-f6bba35b1772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = false)\n",
            " |-- weight: integer (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example4DF = spark.createDataFrame(example, customSchema2)\n",
        "example4DF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5rayh3uQ9Jg",
        "outputId": "1da41ee7-a1d3-485f-b769-55ead0eabdc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'int'>\n",
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "print(type(example2DF.collect()[0][\"weight\"]))\n",
        "print(type(example3DF.collect()[0][\"weight\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QnciaoWQ9Jg"
      },
      "source": [
        "## Exercises\n",
        "1) Create a manual DF describing smartphones\n",
        "  - maker\n",
        "  - model\n",
        "  - screen dimension\n",
        "  - camera megapixels\n",
        "  \n",
        "2) Read another file from the dataset/ folder, e.g. movies.json\n",
        "  - print its schema\n",
        "  - count the number of rows, call count()\n",
        "\n",
        "3) Take a look to vehicles.csv. Read the file to a DF but this time with your own schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQR1cduXQ9Jg"
      },
      "source": [
        "Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customSchema = StructType([ \\\n",
        "    StructField('maker', StringType(), True), \\\n",
        "    StructField('model', StringType(), True), \\\n",
        "    StructField('screen dimension', StringType(), True), \\\n",
        "    StructField('camera megapixels', StringType(), True)])"
      ],
      "metadata": {
        "id": "hDTp36b21T-i"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmg5kbOpl-s3"
      },
      "source": [
        "Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moviesDF = spark.read.option(\"inferSchema\", True).json(\"/dataset/movies.json\")\n",
        "moviesDF.printSchema()\n",
        "moviesDF.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz_gA2_22q41",
        "outputId": "85eaa9ac-ee0c-4cde-cd21-e8df49a7b78f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Creative_Type: string (nullable = true)\n",
            " |-- Director: string (nullable = true)\n",
            " |-- Distributor: string (nullable = true)\n",
            " |-- IMDB_Rating: double (nullable = true)\n",
            " |-- IMDB_Votes: long (nullable = true)\n",
            " |-- MPAA_Rating: string (nullable = true)\n",
            " |-- Major_Genre: string (nullable = true)\n",
            " |-- Production_Budget: long (nullable = true)\n",
            " |-- Release_Date: string (nullable = true)\n",
            " |-- Rotten_Tomatoes_Rating: long (nullable = true)\n",
            " |-- Running_Time_min: long (nullable = true)\n",
            " |-- Source: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- US_DVD_Sales: long (nullable = true)\n",
            " |-- US_Gross: long (nullable = true)\n",
            " |-- Worldwide_Gross: long (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3201"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAqADV5jW00d"
      },
      "source": [
        "Exercise 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "JD2KQtXiYmVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aebe11d-57c8-4647-9bf2-18149351ac6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name,model,manufacturer,cost_in_credits,length,max_atmosphering_speed,crew,passengers,cargo_capacity,consumables,vehicle_class: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vehiclesDF = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(\"/dataset/vehicles.csv\")\n",
        "customSchema = StructType([ \\\n",
        "    StructField('name', StringType(), True), \\\n",
        "    StructField('model', StringType(), True), \\\n",
        "    StructField('manufacturer', StringType(), True), \\\n",
        "    StructField('custom_in_credits', StringType(), True)])\n",
        "vehiclesDF.printSchema()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "94aRTBiPQ9Jc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ff1af5cda0bea4fe5c4ebc1f94ab9f13d8998f98d08e16d8aba48673b9d00116"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}